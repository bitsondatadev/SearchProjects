<DOC>
<DOCNO> http://www.cs.purdue.edu/homes/alanqi/Courses/CS590_2009.html </DOCNO>
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"> <html> <head> <meta http-equiv="content-type" content="text/html; charset=ISO-8859-1"> <title>CS59000: Statistical Machine Learning</title> </head> <body> <h2>Statistical Machine Learning</h2> <p>CS59000-005 / STAT598N &#8226; Fall 2009 &#8226; Time: TR 3:00 pm - 4:15 pm &#8226; Location:&nbsp; Lawson Computer Science Bldg 1106<br> </p> <p><a href="ML-09/lecture-notes.html">Lecture Notes</a><br> </p> <p><span style="font-weight: bold;">This course has been approved by the department as a Qual 1 course for CS graduate students.</span><br> </p> <hr> <h3>Instructor</h3> <p><a href="http://www.cs.purdue.edu/%7Ealanqi">Professor Alan Qi</a> <br> Lawson 2142L &#8226; alanqi[at]cs.purdue.edu<br> Office hours by appointment<br> </p> <h3>Teaching assistant</h3> <p>Yao Zhu<br> Lawson B116F&nbsp; &#8226; zhu36[at]cs.purdue.edu &#8226; <br> Office hours:&nbsp; MW 4:15 pm - 5:30 pm or by appointment </p> <h3>Course Description</h3> <p><big>This introductory </big><span style="font-size: 20pt; font-family: Calibri; color: black;"></span><big><span style="font-family: calibri;"></span>course will cover many concepts, models, and algorithms in machine learning. Topics include classical supervised learning (e.g., regression and classification), unsupervised learning (e.g., principle component analysis and K-means), and recent development in the machine learning field such as variational Bayes, expectation propagation, and Gaussian processes.&nbsp; While this course will give students the basic ideas and intuition behind modern machine learning methods, the underlying theme in the course is probabilistic inference.<br> </big></p> <p></p> <h3>Tentative Topics</h3> Review on probability distributions and basic concepts in information theory<br> <br> Linear regression and classification<br> <br> Probabilistic graphical models: Bayesian networks, Markov random fields and conditional random fields<br> <br> K-means Clustering, mixture models&nbsp;and Expectation Maximization<br> <br> Hidden Markov models, state space models, and forward-backward algorithms<br> <br> Sampling methods, importance sampling, and Markov Chain Monte Carlo<br> <br> Deterministic approximate inference: Laplace's method, Variational Bayes, and expectation propagation<br> <br> Kernel methods<br> <br> Selected topics:<br> &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; Nonparametric Bayesian: Dirichlet process mixture models<br> &nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp; Combining models (or weak learners): Boosting<br> &nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp; Recent papers from NIPS, ICML, UAI, &nbsp;JMLR, etc.<br> <h3>Prerequisites</h3> <p>Calculus, basic linear algebra and probability, or permission of instructor. </p> <h3>Textbooks (recommended)<br> </h3> <p></p> <p></p> <p>Pattern Recognition and Machine Learning, Christopher M. Bishop, 2007<br> <br> Information Theory, Inference, and Learning Algorithms, David MacKay, 2003. Available on-line <a href="http://www.inference.phy.cam.ac.uk/mackay/itila/book.html">here</a>.</p> <h3>Assignments</h3> <ul> <li>Homework (links will be activated as homework is assigned). Copying will not be tolerated.</li> <li>Midterm in mid October </li> <li>Review of recent research <br> Students will choose a subtopic of machine learning research, select three recent conference papers on the topic, and write a 2 page report outlining the main ideas of papers and relate them to the context of the course.&nbsp; </li> <li>Final project:&nbsp; You are required to complete a class project. The choice of the topic is up to you so long as it clearly pertains to the course material. To ensure that you are on the right track, you will have to submit a one paragraph description of your project a month before the project is due. You are encouraged to collaborate on the project, but . We expect a four page write-up about the project, which should clearly and succintly describe the project goal, methods, and your results. Each group should submit only one copy of the write-up and include all the names of the group members (a two person group will have 6 pages, a three person group will have 8 pages, and so on). The projects will be graded on the basis of your understanding of the overall course material (not based on, e.g., how brilliantly your method works). <br> </li> </ul> <h3>Grading</h3> <ul> <li>Class participation: 5% </li> <li>Homework: 25% </li> <li>Midterm: 25% </li> <li>Paper Review: 5% </li> <li>Final project: 10%</li> <li>Final exam: 30%</li> </ul> <h3>Late policy</h3> Assignments will be accepted up to 5 days late with a penalty of 10% per day. No assignment will be accepted more than 5 days late. <big><br> </big> <h3><span style="font-weight: bold;">Emergency policy</span><br> </h3> In the event of a major campus emergency, course requirements, deadlines and grading percentages are subject to changes that may be necessitated by a revised semester calendar or other circumstances beyond the instructor&#8217;s control. Here are ways to get information about changes in this course. Course web page:&nbsp;&nbsp; http://www.cs.purdue.edu/homes/alanqi/Courses/CS590_2009.html<br> Instructor&#8217;s and TA's emails: alanqi [at]cs.purdue.edu or .zhu36 [at]cs.purdue.edu &#8226; <br> <br> To avoid&nbsp; the spread of pandemic influenza, a student is not recommended to come to class with a fever or for seven calendar days after recovering from influenza. Mitigation practices can be found at&nbsp; www.itap.purdue.edu/tlt/faculty.<br> <br> The development of the course material is partially supported by the National Science Foundation under Grant No.&nbsp; 0916443. Any opinions, findings and conclusions or recomendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation (NSF).<br> <br> <hr> <h3><br> </h3> <br> </body> </html> </html>
</DOC>
